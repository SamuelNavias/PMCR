---
title: "Chapter 1: Probability, R, and Monte Carlo Simulation"
output:
  html_document:
    includes:
      before_body: "sidebar.html"
    css: "custom.css"
---


```{r setup, include=FALSE}
# Load the DiagrammeR package
library(DiagrammeR)
```

```{r, echo=FALSE}
# Run the script to automatically commit and push changes after knitting
source("git_push_after_knit.R")
```

### [Back Home](https://samuelnavias.github.io/PMCR/home.html)

## Some Stuff: Probability / R / Monte Carlo

### What is Probability? (The Logic Of Uncertainty)

The mathematical framework for probability is built around **sets**. Imagine that an **experiment** is performed, resulting in one out of a set of possible **outcomes**. Before the experiment is performed, it is unknown which outcome will be the result; after, the result "crystallizes" into the actual outcome.

Let’s think about what that means. Consider the experiment of rolling a standard 6-sided die. There are 6 possible outcomes before we roll, but only one number will face up after the die is tossed.

---

### Dice Example:

- The experiment: Rolling a die and seeing what number pops up  
-- The set of possible outcomes: `{1, 2, 3, 4, 5, 6}`

---

### Some Math: Sample Space and Event

Definitions: 

- **Sample Space**: The set of all possible outcomes of an experiment.  
- **Event**: A subset of the sample space. We say that an event occurred if the actual outcome is in the event.

Back to dice: The sample space for rolling a six-sided die is `{1, 2, 3, 4, 5, 6}`.  
We can say that the event \( A \) is rolling a 1. The probability of this event is:

\[
\text{P}(A) = \frac{\text{Number of favorable outcomes}}{\text{Total number of outcomes}}
\]

# Probability of rolling a 1:

- Consider the set of all possible outcomes `{1, 2, 3, 4, 5, 6}` and the one we are interested in`{1}`.
- By simple counting we can see:

\[
\text{P}(A) = \frac{1}{6}
\]

---

### What is Monte Carlo Simulation?

Monte Carlo Simulation is a computational algorithm that uses repeated random sampling to obtain the likelihood of various results.

We can use Monte Carlo to evaluate the number of favorable outcomes (event A) out of many simulations. Instead of thinking through the outcomes logically, we simulate the experiment multiple times and count how often the event of interest happens.

#### Example: Rolling a die 10,000 times and counting the 1s

# Simulate rolling a die 10,000 times
```{r}
set.seed(123)
rolls <- sample(1:6, size = 10000, replace = TRUE)
table(rolls)

# Count how many times we roll a 1
count_1s <- sum(rolls == 1)
count_1s

# Estimate probability of rolling a 1
estimated_p_1 <- count_1s / 10000
estimated_p_1
# We can look at the difference between our simulated probability and the true one
# Turns out were pretty close 
estimated_p_1 - (1/6)
```

### How Monte Carlo Works

1. Define a Problem: Start with a system or process you want to analyze.
2. Generate Random Inputs: Monte Carlo simulation introduces randomness instead of deterministic inputs.
3. Simulate Multiple Iterations: Run the model multiple times using different random inputs.
4. Analyze the Results: After running thousands of simulations, analyze the distribution of outcomes to gain insights.

---

# R Programming Basics

R is built around vectors, and getting familiar with "vectorized thinking" is very important for using R effectively!

## Creating a vector
```{r}
v <- c(3, 1, 4, 1, 5, 9)
v
```

## Important functions
```{r}
# Sum of the vector
sum(v)

# Maximum value
max(v)  

# Minimum value
min(v)  

# Length of the vector
length(v)  
```

# More R: Sequences and Subsetting

### Sequence from 1 to 100
```{r}
v_seq <- 1:100
v_seq

# Accessing specific elements
v_subset <- v[c(1, 3, 5)]  # 1st, 3rd, and 5th elements
v_subset

# Exclude elements
v_exclude <- v[-(2:4)]  # Exclude the 2nd to 4th elements
v_exclude
```

# Using sample() in R

The sample() function allows you to draw random samples in R.  
For example, drawing a random sample of size 5 from the numbers 1 to 10:

```{r}
sample(1:10, 5)
```

## Monte Carlo Example: Coin Flipping

Let's simulate flipping a coin 10,000 times and observe the outcomes.

```{r}
set.seed(123)
coin_flips <- replicate(10000, sample(c("Heads", "Tails"), 1, replace = TRUE))
f_table <- table(coin_flips)

barplot(
  f_table,
  main = "Frequency of Each Outcome",
  xlab = "Outcome (Heads or Tails)",
  ylab = "Frequency",
  col = "lightblue",
  border = "black"
)
```

<hr style="border: 2px solid black;">

# What is a Discrete Random Variable?

So far, we’ve talked about experiments like rolling dice and flipping coins. In each case, we observed a set of possible outcomes, and we assigned probabilities to those outcomes.

Now, we can group these ideas under a new term: **discrete random variables**.

### Here's what that means:

1. **Random Variable**: 
   - A random variable is just a way to assign a number to each possible outcome of an experiment.
   - Think of it like this: Instead of saying “I rolled a 6 on the die,” we can just say “The random variable equals 6.”
   - It’s just a number that represents the outcome of a random process.

2. **Discrete**:
   - The word **discrete** simply means that the possible values the random variable can take are distinct and separate.
   - In other words, the random variable can only take specific values (e.g., 1, 2, 3, etc.) with nothing in between.
   
### Example: Rolling a Die
- When you roll a die, the random variable represents the numbers you can roll: 1, 2, 3, 4, 5, or 6.
- These are discrete values because there are no possible outcomes between them (you can't roll a 2.5 on a die).

---

### How Does This Relate to Probability?

For every value the random variable can take, there is a corresponding **probability** that it will happen. These probabilities must add up to 1, just like we discussed before.

### Example: Rolling a Die
- We can think of the roll of a die as a discrete random variable, let’s call it \( X \).
- The possible values of \( X \) are: {1, 2, 3, 4, 5, 6}.
- The probability of each value is the same: \( \text{P}(X = 1) = \frac{1}{6} \), \( \text{P}(X = 2) = \frac{1}{6} \), and so on.

---

### Key Points:

- A **discrete random variable** takes on distinct, separate values, like the result of rolling a die or flipping a coin.
- For each possible value, there is a **probability** that tells us how likely that value is to occur.
- All of the probabilities for the random variable’s values must add up to **1** (since something has to happen).

---

# Expectation

## What is Expectation (Expected Value)?

Now that we know what a **discrete random variable** is, we can talk about a key concept in probability: the **expected value** of a random variable.

### What Does Expectation Mean?

The expected value, often written as \( \text{E}(X) \), is the **average** value we would expect to get if we repeated the random experiment many times. It’s like a long-term **weighted average** of all possible outcomes.

### Here's the basic idea:

- For each possible outcome of the random variable \( X \), we multiply the **value of the outcome** by the **probability** that it happens.
- Then, we **sum up** these products.

So, the expectation takes into account both the **value** of each outcome and how **likely** it is. This helps give us an idea of the average result over the long run.

---

### Example: Rolling a Weighted Die

Imagine we have a **weighted die** where:
- The probability of rolling a 6 is 50% (or \( P(X = 6) = 0.5 \)).
- The probabilities of rolling 1, 2, 3, 4, or 5 are all equal and add up to the remaining 50%.

Let's calculate the **expected value** of this weighted die. We'll use the following formula:

\[
\text{E}(X) = \sum \left( \text{outcome} \times \text{probability of outcome} \right)
\]

<div style="text-align: center;">
(*To clarify $\sum$ is just summation notation so you add the stuff that is inside the sum*)
</div>

In this case:

\[
\text{E}(X) = (1 \times 0.1) + (2 \times 0.1) + (3 \times 0.1) + (4 \times 0.1) + (5 \times 0.1) + (6 \times 0.5)
\]

\[
\text{E}(X) = 0.1 + 0.2 + 0.3 + 0.4 + 0.5 + 3 = 4.5
\]

So because it’s a **weighted die**, the **expected value** (or average outcome over many rolls) is 4.5 as opposed to what we would expect in an non-weighted die where the probability of each outcome is equal.

**If the die was not weighted what would the expected value be?**

---

### Why Is This Useful?

- The **expected value** helps us understand what we would expect to happen on average over a large number of trials.
- Even though any individual trial can produce a different outcome, the expected value gives us a kind of "central tendency" for the random variable.
  
---

### Key Points about Expectation:

1. **Expected value** is like the long-term average of many trials.
2. It’s calculated by multiplying each possible outcome by its probability, then adding them up.
3. The expected value doesn’t have to be one of the possible outcomes! In the example above, you can’t roll a 4.5 on a die, but that’s the expected value.

## Mathematical Definition of Expectation for a Discrete Random Variable

For a **discrete random variable** \( X \), the expected value is given by the formula:

\[
E(X) = \sum_{i} x_i P(X = x_i)
\]

Where:
- \( x_i \) are the possible outcomes of the random variable.
- \( P(X = x_i) \) is the probability of each outcome \( x_i \).

---

# Variance

## What is Variance?

Now that we know how to find the **expected value** of a random variable, we can ask another important question: 
**How far do the outcomes typically deviate from the expected value?**

This brings us to the concept of **variance**.

---

### Here's what variance means:

- **Variance** measures how spread out the values of a random variable are around the expected value.
- If the values tend to be close to the expected value, the variance is small.
- If the values tend to be far from the expected value, the variance is large.

In simpler terms, variance tells us how much the random variable **varies** from its expected value on average.

---

### How is Variance Calculated?

To calculate the variance, we follow these steps:

1. First, we find the **difference** between each possible outcome and the expected value. This tells us how far each outcome is from what we expect on average.
   
   - Difference: \( x_i - E(X) \), where \( x_i \) is the outcome and \( E(X) \) is the expected value.
   
2. Next, we **square** these differences. This ensures all the deviations are positive.
   
   - Squared difference: \( (x_i - E(X))^2 \)
   
3. Finally, we calculate the **expected value** of the squared differences. This gives us the average squared deviation, which is the variance.
   
   - Variance: \( \text{Var}(X) = E\left[(X - E(X))^2\right] \)

---

### Example: Variance of a Weighted Die

Let's go back to our **weighted die** example. We already calculated the **expected value** \( E(X) = 4.5 \).

Now, let's calculate the **variance**:

\[
\text{Var}(X) = E\left[(X - 4.5)^2\right]
\]

We do this by following the same process as for expectation, but now we multiply each outcome's squared deviation by its probability:

\[
\text{Var}(X) = (1 - 4.5)^2 \times 0.1 + (2 - 4.5)^2 \times 0.1 + \dots + (6 - 4.5)^2 \times 0.5
\]

This gives us a measure of how much the outcomes of this weighted die typically vary from the expected value of 4.5.

---

### Why is Variance Important?

- **Variance** tells us how much variability there is in a random variable. It helps us understand whether the outcomes are tightly clustered around the expected value or if they are spread out.
- A small variance means most outcomes are close to the expected value, while a large variance means outcomes vary widely from the expected value.

---

## Mathematical Definition of Variance

For a **discrete random variable** \( X \), the variance is given by the formula:

\[
\text{Var}(X) = \text{E}\left[(X - \text{E}(X))^2\right] = \sum_{i} (x_i - \text{E}(X))^2 P(X = x_i)
\]

Where:
- \( x_i \) are the possible outcomes of the random variable.
- \( P(X = x_i) \) is the probability of each outcome \( x_i \).
- \( E(X) \) is the expected value of the random variable.

<div style="text-align: center;">
(**There is a more handy formula that is equivalent:**)
</div>

\[
\text{Var}(X) = \text{E}[X^2] - \text{E}[X]^2
\]


<hr style="border: 2px solid black;">

# Now we'll go back to R to practice what we've learned

### Sticking with the example from before of the weighted die we'll calculate the EV and Variance using Monte Carlo

```{r}
# Writing a function to roll the weighted die
rollD <- function(){
  r <- sample(6, 1, prob = c(0.1,0.1,0.1,0.1,0.1,0.5))
  return(r)
}

#running one hundred thousand simulation of the weighted die roll
roll_simz <- replicate(100000, rollD())

#retrieving the frequency of each outcome
f_table_roll <- table(roll_simz)

#looking at the values of each frequency
f_table_roll

#plotting the distribution
barplot(
  f_table_roll,
  main = "Frequency of Each Outcome",
  xlab = "Outcome (Heads or Tails)",
  ylab = "Frequency",
  col = "lightblue",
  border = "black"
)
```

### NOW WE CALCULATE EV
```{r}
# Extract the outcomes (1, 2, 3, 4, 5, 6)
outcomes <- as.numeric(names(f_table_roll))

# Extract the frequencies
frequencies <- as.numeric(f_table_roll)

# Calculate the probabilities (frequencies divided by total number of rolls)
probabilities <- frequencies / sum(frequencies)

# Calculate the expected value
EV <- sum(outcomes * probabilities)
EV
```

### NOW WE CAN CALCULATE THE VARIANCE
#### We can refer back to the formula from before:

\[
\text{Var}(X) = \text{E}[X^2] - \text{E}[X]^2
\]

<div style="text-align: center;">
(**Note that this uses the expected value which we have already calculated**)
</div>

```{r}
outcomes_sq = outcomes^2

EX2 <- sum(outcomes_sq * probabilities)

var_roll <- EX2 - EV^2

var_roll
```

<hr style="border: 2px solid black;">

## Normal Distribution: A Simple Explanation

The **normal distribution** (also known as the **Gaussian distribution**) is one of the most commonly used probability distributions in statistics. It has the following characteristics:

1. **Symmetrical Bell Curve**: The distribution has a symmetric shape around its mean, where most of the data points are close to the mean, and fewer are farther away. The further you go from the mean, the less likely the values are.
   
2. **Mean and Standard Deviation**:
   - The **mean ($\mu$)** represents the "center" or the average of the distribution.
   - The **standard deviation ($\sigma$)** represents how spread out the data is. A larger standard deviation means more variability, and a smaller standard deviation means the data is more tightly packed around the mean.

3. **68-95-99.7 Rule**: 
   - About **68%** of the data falls within one standard deviation of the mean.
   - About **95%** falls within two standard deviations.
   - About **99.7%** falls within three standard deviations.

The **normal distribution** is crucial because many real-world phenomena (like heights, test scores, or returns in financial markets) approximate this distribution under certain conditions.

### Visualizing the Normal Distribution

A standard normal distribution (mean = 0, standard deviation = 1) looks like this:

```{r, echo = FALSE}
# Generate a sequence of x values (from -4 to 4)
x <- seq(-4, 4, length=100)

# Calculate the density of the standard normal distribution
y <- dnorm(x)

# Plot the standard normal distribution
plot(x, y, type="l", lwd=2, col="blue", 
     main="Standard Normal Distribution", 
     xlab="Z-scores", ylab="Density")

# Add vertical lines at -1, 0, 1 for reference (mean and 1 std deviation)
abline(v = c(-1, 0, 1), col="red", lty=2)
```

\[
\text{This is the PDF of the standard normal: }f(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}
\]

\[
\text{This is the PDF of the normal: }f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]

<div style="text-align: center;">

Where: $\mu$ is the mean, $\sigma$ is the standard deviation, $e$ is Euler's number.

A **probability density function (PDF)** describes the likelihood of a **continuous** random variable taking on different values. While the value of the PDF itself is not a probability, the area under the curve between two points represents the probability that the random variable falls within that range.

**The important thing to note is that unlike the discrete random varibales we talked about you can't evaluate the probabilty that a continuous random variable will take a certain value, but you consider the probability that the continuous random variable will fall within a given range of values.**

</div>

---

In R, the **`pnorm`** and **`qnorm`** functions help you work with the normal distribution:

### **`pnorm(x)`**: 
   - This function gives the **cumulative probability** up to a point `x` for a normal distribution.
   - In other words, it tells you the probability that a random variable from the normal distribution will be **less than or equal** to `x`.
   - Think of it as the area under the curve of the normal distribution to the left of `x`.

```{r, echo = FALSE}
# Plot the standard normal distribution
plot(x, y, type="l", lwd=2, col="blue", 
     main="Standard Normal Distribution with Shaded Area", 
     xlab="Z-scores", ylab="Density")

# Shade the area under the curve up to Z = 1
x_shade <- seq(-4, 1.96, length=100)
y_shade <- dnorm(x_shade)
polygon(c(-4, x_shade, 1.96), c(0, y_shade, 0), col="lightblue", border=NA)

# Re-draw the normal curve
lines(x, y, lwd=2, col="blue")

# Add vertical lines at -1, 0, and 1 for reference
abline(v = c(1.96), col="red", lty=2)
## `pnorm` and `qnorm` Functions in R
```

   **Example**:
   # Probability that a value is less than or equal to 1.96 in a standard normal distribution

```{r}   
pnorm(1.96)
```
   This returns **0.975**, which means that 97.5% of the values lie below 1.96 in a standard normal distribution.

---

### **`qnorm(p)`**:
   - This is the **inverse** of `pnorm()`. Given a cumulative probability `p`, `qnorm()` returns the value of `x` such that the area to the left of `x` is `p`.
   - In simpler terms, it finds the value of `x` that corresponds to a given probability in the normal distribution.

```{r, echo = FALSE}
# Plot the standard normal distribution
plot(x, y, type="l", lwd=2, col="blue", 
     main="Standard Normal Distribution with Shaded Area", 
     xlab="Z-scores", ylab="Density")

# Shade the area under the curve up to Z = 1
x_shade <- seq(-4, -1.645, length=100)
y_shade <- dnorm(x_shade)
polygon(c(-4, x_shade, -1.645), c(0, y_shade, 0), col="salmon", border=NA)

# Re-draw the normal curve
lines(x, y, lwd=2, col="blue")

# Add vertical lines at -1, 0, and 1 for reference
abline(v = c(-1.645), col="lightblue", lty=2)
## `pnorm` and `qnorm` Functions in R
```

   **Example**:
   # Find the value of x such that 5% of the values lie below it in a standard normal distribution

```{r}
qnorm(0.05)
```

   This returns **-1.645**, which means that 5% of the values are less than or equal to -1.645 in a standard normal distribution.

---

## Example of How to Use `pnorm` and `qnorm` in Practice

### Using `pnorm`:

Suppose you have a dataset with a mean of 100 and a standard deviation of 15 (like IQ scores). You want to know the probability that someone has an IQ less than 120.

```{r}
mean <- 100
sd <- 15

# Probability that IQ is less than 120
pnorm(120, mean = mean, sd = sd)
```

This will return a value close to **0.908**, meaning about 90.8% of people have an IQ below 120.

---

### Using `qnorm`:

Now, suppose you want to find the IQ score that corresponds to the top 5% (the 95th percentile).

#### Find the IQ score that corresponds to the 95th percentile

```{r}
qnorm(0.95, mean = mean, sd = sd)
```

This will return approximately **124.67**, meaning that an IQ of about 124.67 is higher than 95% of the population.

---

## Summary:
- **Normal Distribution**: A bell-shaped, symmetrical distribution described by its mean and standard deviation.
- **`pnorm(x)`**: Returns the probability that a value from the normal distribution is less than or equal to `x`.
- **`qnorm(p)`**: Returns the value `x` such that the probability of a value being less than or equal to `x` is `p`.
